## model architecture configurations
d_model: 768 # size of embedding dimension
drop: null # if not null pass float number between 0 to 1
eps: 0.00001 # added denominator to avoid devided by zero error
N: 12 # number of attention modules attached 
vocab_size: 50257 # size of word curpos 
seq_len: 1024 # max sequence length
mlp_scale: 4 # times the intermediate state of the MLP neural network layer

position_embedding_type: 'standard' # 'rotary', 'transformer'
base: 10000 # base of angle exponent for rotary position embedding


## attention configurations
head: 12 # number of attention heads
attention_type: 'transformer' # 'sliding-window', 'transformer'
groups: 2 # for group-query attention
context: 3 # for sliding-window attention


## training configurations
batch_size: 10 # batch size for training/testing
optimizer: 'adam' # optimizer for weight updates
